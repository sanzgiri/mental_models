# Existential Risks

 **Existential Risks: Understanding the Potential Peril of Artificial Intelligence**

Existential risks refer to scenarios where substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe. This mental model is crucial as it highlights a potential future threat that could have drastic consequences for humanity's survival.

**Core Concept**

The concern revolves around the possibility of AI surpassing human intelligence, becoming superintelligent, and potentially becoming uncontrollable. Similar to the mountain gorilla's fate depending on human goodwill, the future of humanity could be at the mercy of a future machine superintelligence. While there is ongoing debate about the technical feasibility, speed of self-improvement, and effectiveness of alignment strategies, many experts agree that the risk cannot be entirely dismissed.

**Examples**

1. In 2023, hundreds of AI experts and other notable figures signed a statement declaring that mitigating the risk of extinction from AI should be a global priority.
2. Elon Musk, founder of SpaceX and Tesla, has been vocal about his concerns regarding AI, stating that "with artificial intelligence we are summoning the demon."
3. In 2022, a survey of AI researchers found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.

**Application**

While it's challenging to predict when AGI or superintelligence may be achieved, understanding the existential risks associated with these advancements can help society prepare and make informed decisions about AI development. This includes investing in research on safety and alignment, advocating for global regulation of AI, and encouraging transparency and ethical considerations in AI development.

**Related Models**

1. **The Singularity**: This mental model posits that AGI will rapidly improve upon itself, leading to an "intelligence explosion" that outpaces human comprehension and control.
2. **AI Control Problem**: This model focuses on finding ways to ensure that AI systems are aligned with human values and goals, preventing them from becoming a threat to humanity.
3. **Technological Unemployment**: As AI advances, it may automate many jobs, leading to unemployment and social upheaval. Understanding this risk can help us prepare for potential changes in the job market and develop strategies for adaptation.

**Sources:**
1. Existential risk from artificial intelligence - Wikipedia, The Free Encyclopedia (2023) [Accessed: 24 March 2023] <https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence>
2. Superintelligence: Paths, Dangers, Strategies - Nick Bostrom (2014) [Accessed: 24 March 2023] <https://www.technologyreview.com/s/527686/superintelligence-paths-dangers-strategies/>